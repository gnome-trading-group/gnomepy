{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnomepy.backtest.backtest import *\n",
    "from gnomepy.backtest.strategy import *\n",
    "from gnomepy.backtest.archive.signal import *\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from gnomepy.backtest.coint_testing import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Cointegration Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Data from Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MarketDataClient(bucket=\"gnome-market-data-dev\", aws_profile_name=\"AWSAdministratorAccess-443370708724\")\n",
    "client_data_params = {\n",
    "    \"exchange_id\": 1,\n",
    "    \"security_id\": 1,\n",
    "    \"start_datetime\": datetime.datetime(2025, 6, 7),\n",
    "    \"end_datetime\": datetime.datetime(2025, 6, 8),\n",
    "    \"schema_type\": SchemaType.MBP_10,\n",
    "}\n",
    "data = client.get_data(**client_data_params).to_df()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manufacture Synthetic Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average spread between ask and bid\n",
    "avg_spread = (data['askPrice0'] - data['bidPrice0']).mean()\n",
    "\n",
    "# Create lagged signals with different lag lengths\n",
    "lag_lengths = [10, 200, 600]  # Short, medium and long lags\n",
    "n = len(data)\n",
    "\n",
    "# Initialize perturbed series with original prices (bidPrice0)\n",
    "data['bidPrice0_perturbed_1'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "data['bidPrice0_perturbed_2'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "data['bidPrice0_perturbed_3'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "\n",
    "# Initialize perturbed ask prices by adding spread to perturbed bid prices\n",
    "data['askPrice0_perturbed_1'] = data['bidPrice0_perturbed_1'] + avg_spread\n",
    "data['askPrice0_perturbed_2'] = data['bidPrice0_perturbed_2'] + avg_spread\n",
    "data['askPrice0_perturbed_3'] = data['bidPrice0_perturbed_3'] + avg_spread\n",
    "\n",
    "# --- New: Create random normal walk and its perturbed versions ---\n",
    "# Create a random walk series\n",
    "data['bidPrice0_random_normal'] = np.cumsum(np.random.normal(size=len(data))) + 10000\n",
    "data['askPrice0_random_normal'] = data['bidPrice0_random_normal'] + avg_spread\n",
    "\n",
    "# Initialize perturbed versions of the random walk\n",
    "data['bidPrice0_random_normal_perturbed_1'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "data['bidPrice0_random_normal_perturbed_2'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "data['bidPrice0_random_normal_perturbed_3'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "\n",
    "# Initialize random_normal_perturbed ask prices by adding spread to random_normal_perturbed bid prices\n",
    "data['askPrice0_random_normal_perturbed_1'] = data['bidPrice0_random_normal_perturbed_1'] + avg_spread\n",
    "data['askPrice0_random_normal_perturbed_2'] = data['bidPrice0_random_normal_perturbed_2'] + avg_spread\n",
    "data['askPrice0_random_normal_perturbed_3'] = data['bidPrice0_random_normal_perturbed_3'] + avg_spread\n",
    "\n",
    "# Generate random lag points for each series\n",
    "num_lags = n // 1000  # Create lags roughly every 1000 ticks\n",
    "lag_points = np.sort(np.random.choice(range(n-max(lag_lengths)), num_lags, replace=False))\n",
    "\n",
    "# Apply lags at random points for both bidPrice0 and bidPrice0_random_normal perturbed series\n",
    "for start_idx in lag_points:\n",
    "    # For each lag length (short, medium, long)\n",
    "    for i, lag in enumerate(lag_lengths, 1):\n",
    "        # --- For bidPrice0 perturbed series ---\n",
    "        lagged_bid_price = data['bidPrice0'].iloc[start_idx-lag:start_idx-lag+lag].values\n",
    "        lagged_ask_price = lagged_bid_price + avg_spread\n",
    "        data[f'bidPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_bid_price\n",
    "        data[f'askPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_ask_price\n",
    "\n",
    "        # --- For bidPrice0_random_normal perturbed series (bid and ask) ---\n",
    "        lagged_random_normal_bid = data['bidPrice0_random_normal'].iloc[start_idx-lag:start_idx-lag+lag].values\n",
    "        lagged_random_normal_ask = lagged_random_normal_bid + avg_spread\n",
    "        data[f'bidPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_bid\n",
    "        data[f'askPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_ask\n",
    "\n",
    "# Calculate log of prices\n",
    "data['log_price'] = np.log(data['bidPrice0'])\n",
    "data['log_price_perturbed_1'] = np.log(data['bidPrice0_perturbed_1'])\n",
    "data['log_price_perturbed_2'] = np.log(data['bidPrice0_perturbed_2'])\n",
    "data['log_price_perturbed_3'] = np.log(data['bidPrice0_perturbed_3'])\n",
    "\n",
    "# Also calculate log of random normal and its perturbed versions\n",
    "data['log_price_random_normal'] = np.log(data['bidPrice0_random_normal'])\n",
    "data['log_price_random_normal_perturbed_1'] = np.log(data['bidPrice0_random_normal_perturbed_1'])\n",
    "data['log_price_random_normal_perturbed_2'] = np.log(data['bidPrice0_random_normal_perturbed_2'])\n",
    "data['log_price_random_normal_perturbed_3'] = np.log(data['bidPrice0_random_normal_perturbed_3'])\n",
    "\n",
    "# Drop NaN values from all columns used in plotting\n",
    "data = data.dropna(subset=[\n",
    "    'log_price', 'log_price_perturbed_1', 'log_price_perturbed_2', 'log_price_perturbed_3',\n",
    "    'log_price_random_normal', 'log_price_random_normal_perturbed_1', 'log_price_random_normal_perturbed_2', 'log_price_random_normal_perturbed_3'\n",
    "], axis=0)\n",
    "\n",
    "# Display the log prices, plotting every 1000th point\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price'][::1000], label='Original Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_1'][::1000], label='Perturbed 1% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_2'][::1000], label='Perturbed 2% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_3'][::1000], label='Perturbed 3% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal'][::1000], label='Random Normal Log Price', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_1'][::1000], label='Random Normal Perturbed 1', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_2'][::1000], label='Random Normal Perturbed 2', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_3'][::1000], label='Random Normal Perturbed 3', alpha=0.8, linestyle='--')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Log Price')\n",
    "# plt.title('Log of Original, Perturbed, and Random Normal bidPrice0')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Cointegrated Baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_baskets, cointegrated_baskets = get_coint_baskets(\n",
    "    columns=[\n",
    "        'bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_2', 'bidPrice0_perturbed_3', \n",
    "        'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2',\n",
    "        'bidPrice0_random_normal_perturbed_3'\n",
    "    ], \n",
    "    data=data, significance_level=0.10, min_basket_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basksets from cointegration testing\n",
    "baskets = list(cointegrated_baskets.keys())\n",
    "\n",
    "# Your setup code here\n",
    "results = main(\n",
    "    baskets=baskets,\n",
    "    data=data,\n",
    "    beta_refresh_freq=1000,\n",
    "    spread_window=100,\n",
    "    cash_start=10000,\n",
    "    notional=100,\n",
    "    trade_freq=1,\n",
    "    execution_delay=0,\n",
    "    enter_zscore=2.5,\n",
    "    exit_zscore=0.1,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "backtest_results = {basket: {'history_df': history_df, 'trade_log': trade_log}\n",
    "                    for basket, history_df, trade_log in results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_backtest_summary(history_df, trade_log):\n",
    "    import numpy as np\n",
    "\n",
    "    # Compute average profit per complete trade and win ratio\n",
    "    # We assume trade_log has columns: 'action', 'before_cash', 'after_cash', etc.\n",
    "    # We'll look for pairs of (enter_long/enter_short) followed by (exit_long/exit_short)\n",
    "    complete_trade_profits = []\n",
    "    complete_trade_durations = []\n",
    "    open_trade = None\n",
    "    open_trade_idx = None\n",
    "\n",
    "    for idx, row in trade_log.iterrows():\n",
    "        action = row['action']\n",
    "        if action in ['enter_long', 'enter_short']:\n",
    "            # Start of a new trade\n",
    "            open_trade = row\n",
    "            open_trade_idx = idx\n",
    "        elif action in ['exit_long', 'exit_short'] and open_trade is not None:\n",
    "            # End of a trade, compute profit and duration\n",
    "            profit = row['after_cash'] - open_trade['before_cash']\n",
    "            complete_trade_profits.append(profit)\n",
    "            # Calculate ticks between entry and exit (assume index is ordered)\n",
    "            duration = row['ticks_since_entry']\n",
    "            complete_trade_durations.append(duration)\n",
    "            open_trade = None  # Reset for next trade\n",
    "            open_trade_idx = None\n",
    "\n",
    "    if complete_trade_profits:\n",
    "        avg_profit_per_trade = np.round(np.mean(complete_trade_profits), 3)\n",
    "        std_profit_per_trade = np.round(np.std(complete_trade_profits), 3)\n",
    "        num_wins = np.sum(np.array(complete_trade_profits) > 0)\n",
    "        win_ratio = np.round(num_wins / len(complete_trade_profits), 3)\n",
    "        total_profit = np.round(np.sum(complete_trade_profits), 3)\n",
    "        # Profit Factor: sum of profits over sum of losses (absolute value)\n",
    "        profits = np.array(complete_trade_profits)\n",
    "        gross_profit = np.round(profits[profits > 0].sum(), 3)\n",
    "        gross_loss = np.round(-profits[profits < 0].sum(), 3)  # make positive\n",
    "        if gross_loss > 0:\n",
    "            profit_factor = np.round(gross_profit / gross_loss, 3)\n",
    "        else:\n",
    "            profit_factor = np.nan if gross_profit == 0 else np.inf\n",
    "    else:\n",
    "        avg_profit_per_trade = np.nan  # No complete trades\n",
    "        std_profit_per_trade = np.nan\n",
    "        win_ratio = np.nan\n",
    "        total_profit = np.nan\n",
    "        profit_factor = np.nan\n",
    "\n",
    "    if complete_trade_durations:\n",
    "        avg_ticks_per_trade = np.round(np.mean(complete_trade_durations), 3)\n",
    "        std_ticks_per_trade = np.round(np.std(complete_trade_durations), 3)\n",
    "    else:\n",
    "        avg_ticks_per_trade = np.nan\n",
    "        std_ticks_per_trade = np.nan\n",
    "\n",
    "    # --- Max Drawdown Calculation ---\n",
    "    # For max drawdown, always use 'after_cash' from trade_log\n",
    "    max_drawdown = np.nan\n",
    "    if trade_log is not None and not trade_log.empty and 'after_cash' in trade_log.columns:\n",
    "        values = trade_log['after_cash'].values\n",
    "        running_max = np.maximum.accumulate(values)\n",
    "        drawdowns = (values - running_max) / running_max\n",
    "        max_drawdown = np.round(np.min(drawdowns), 3)\n",
    "\n",
    "    summary = {\n",
    "        'num_complete_trades': len(complete_trade_profits),\n",
    "        'avg_profit_per_complete_trade': avg_profit_per_trade,\n",
    "        'std_profit_per_complete_trade': std_profit_per_trade,\n",
    "        'total_profit': total_profit,\n",
    "        'avg_ticks_per_complete_trade': avg_ticks_per_trade,\n",
    "        'std_ticks_per_complete_trade': std_ticks_per_trade,\n",
    "        'win_ratio': win_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'profit_factor': profit_factor\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Generate summary statistics for each basket\n",
    "backtest_summaries = {}\n",
    "for basket, result in backtest_results.items():\n",
    "    history_df = result['history_df']\n",
    "    trade_log = result['trade_log']\n",
    "    summary = compute_backtest_summary(history_df, trade_log)\n",
    "    backtest_summaries[str(basket)] = summary\n",
    "\n",
    "# Optionally, display as DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame.from_dict(backtest_summaries, orient='index')\n",
    "\n",
    "# Round all float columns to 3 decimals for display\n",
    "float_cols = summary_df.select_dtypes(include=['float', 'float64']).columns\n",
    "summary_df[float_cols] = summary_df[float_cols].round(3)\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_history = np.array([[float(obj) if type(obj) is not int and isinstance(obj, (np.matrix, np.float64)) else obj for obj in element] for element in history])\n",
    "new_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(new_history[:,0], new_history[:,1])\n",
    "# plt.plot(new_history[:,0], new_history[:,2])\n",
    "# plt.plot(new_history[:,0], new_history[:,3])\n",
    "plt.plot(new_history[:,0], new_history[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(new_history[:100,0], new_history[:100,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_basket_matrix = np.column_stack([data['log_price'].values[::100], data['log_price_perturbed_1'].values[::100], data['log_price_perturbed_2'].values[::100], data['log_price_perturbed_3'].values[::100]])\n",
    "# Calculate correlation matrix for the log prices\n",
    "corr_matrix = np.corrcoef(coin_basket_matrix.T)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "corr_df = pd.DataFrame(\n",
    "    corr_matrix, \n",
    "    columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'],\n",
    "    index=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3']\n",
    ")\n",
    "\n",
    "print(\"\\nCorrelation Matrix of Log Prices:\")\n",
    "print(corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# Create dataframe of all the relevant coins we would like to run johansen test on\n",
    "coin_basket_matrix = np.column_stack([data['log_price'].values, data['log_price_perturbed_1'].values, data['log_price_perturbed_2'].values, data['log_price_perturbed_3'].values])\n",
    "coin_basket_df = pd.DataFrame(coin_basket_matrix, columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'])\n",
    "\n",
    "# Run johansen test\n",
    "johansen_result = coint_johansen(coin_basket_matrix, det_order=0, k_ar_diff=1)\n",
    "\n",
    "# Compare trace statistics against critical values\n",
    "# If trace stat > critical value, reject null hypothesis and conclude more cointegrating relationships\n",
    "trace_stats = johansen_result.lr1\n",
    "cv_95 = johansen_result.cvt[:, 1]  # 95% critical values\n",
    "\n",
    "num_coints = 0\n",
    "for i in range(len(trace_stats)):\n",
    "    if trace_stats[i] > cv_95[i]:\n",
    "        num_coints += 1\n",
    "\n",
    "beta_vectors = johansen_result.evec[:, :num_coints]\n",
    "beta_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cointegrated series\n",
    "spread = coin_basket_matrix @ beta_vectors\n",
    "z_score = (spread - spread.mean()) / spread.std()\n",
    "\n",
    "# Plot the cointegrated series\n",
    "plt.figure(figsize=(14, 7))\n",
    "# for i in range(cointegrated_series.shape[1]):\n",
    "plt.plot(spread[::4000, 0], label=f'Cointegrated Series {i+1}')\n",
    "plt.plot(z_score[::4000, 0], label=f'ZScore {i+1}')\n",
    "plt.title('Cointegrated Price Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cointegration of the perturbed signals using Johansen cointegration test:\n",
    "\n",
    "print(\"COINTEGRATION ANALYSIS OF PERTURBED SIGNALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import required libraries for cointegration tests\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Get the price series for cointegration testing\n",
    "original_prices = backtest_3_df[0][0]['bidPrice0'].values[:100000]\n",
    "perturbed_1 = backtest_3_df[0][0]['bidPrice0_perturbed_1'].values[:100000]\n",
    "perturbed_2 = backtest_3_df[0][0]['bidPrice0_perturbed_2'].values[:100000]\n",
    "perturbed_3 = backtest_3_df[0][0]['bidPrice0_perturbed_3'].values[:100000]\n",
    "\n",
    "print(\"\\n1. JOHANSEN COINTEGRATION TEST:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Measure timing for Johansen test\n",
    "johansen_start_time = time.time()\n",
    "\n",
    "# Prepare data for Johansen test (all series together)\n",
    "# We have 4 vectors because we're testing 4 time series:\n",
    "# 1. Original bidPrice0 (the baseline price series)\n",
    "# 2. Perturbed_1 (original + 0.05% noise)\n",
    "# 3. Perturbed_2 (original + 10% noise) \n",
    "# 4. Perturbed_3 (original + 15% noise)\n",
    "# Each vector represents one price series, and we want to test if they move together long-term\n",
    "data_matrix = np.column_stack([original_prices, perturbed_1, perturbed_2, perturbed_3])\n",
    "data_df = pd.DataFrame(data_matrix, columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'])\n",
    "\n",
    "print(f\"Testing cointegration among {data_matrix.shape[1]} price series (vectors):\")\n",
    "print(f\"  - Original bidPrice0\")\n",
    "print(f\"  - Perturbed version 1 (0.05% noise)\")\n",
    "print(f\"  - Perturbed version 2 (10% noise)\")\n",
    "print(f\"  - Perturbed version 3 (15% noise)\")\n",
    "print(f\"Each series has {data_matrix.shape[0]} observations\\n\")\n",
    "\n",
    "# Perform Johansen test\n",
    "johansen_result = coint_johansen(data_matrix, det_order=0, k_ar_diff=1)\n",
    "\n",
    "johansen_end_time = time.time()\n",
    "johansen_duration = johansen_end_time - johansen_start_time\n",
    "\n",
    "# Interpret Johansen test results\n",
    "print(f\"Test completed in {johansen_duration:.3f} seconds\\n\")\n",
    "\n",
    "# Check for cointegration at 5% significance level\n",
    "trace_stats = johansen_result.lr1\n",
    "critical_values_5pct = johansen_result.cvt[:, 1]  # 5% critical values\n",
    "\n",
    "print(\"JOHANSEN TEST RESULTS:\")\n",
    "print(\"Trace Statistics vs Critical Values (5% significance):\")\n",
    "for i in range(len(trace_stats)):\n",
    "    is_cointegrated = trace_stats[i] > critical_values_5pct[i]\n",
    "    print(f\"  H{i}: {trace_stats[i]:.2f} vs {critical_values_5pct[i]:.2f} - {'COINTEGRATED' if is_cointegrated else 'NOT COINTEGRATED'}\")\n",
    "\n",
    "# Count cointegrating relationships\n",
    "num_coint_relationships = sum(trace_stats > critical_values_5pct)\n",
    "\n",
    "print(f\"\\nCOINTEGRATING VECTORS (Eigenvectors):\")\n",
    "print(\"Note: Each eigenvector shows the linear combination weights for the 4 price series\")\n",
    "signal_names = ['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3']\n",
    "for i in range(num_coint_relationships):\n",
    "    print(f\"\\nCointegrating Relationship {i+1}:\")\n",
    "    eigenvector = johansen_result.evec[:, i]\n",
    "    for j, name in enumerate(signal_names):\n",
    "        print(f\"  {name}: {eigenvector[j]:.4f}\")\n",
    "    \n",
    "    # Identify which signals are most strongly related in this relationship\n",
    "    abs_weights = np.abs(eigenvector)\n",
    "    dominant_signals = [signal_names[k] for k in np.where(abs_weights > 0.1)[0]]\n",
    "    print(f\"  Dominant signals in this relationship: {', '.join(dominant_signals)}\")\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Number of cointegrating relationships found: {num_coint_relationships}\")\n",
    "\n",
    "if num_coint_relationships == 4:\n",
    "    print(\"RESULT: All signals are cointegrated - they form a complete cointegrated system.\")\n",
    "    print(\"This means all perturbed versions maintain long-term equilibrium with the original.\")\n",
    "elif num_coint_relationships > 0:\n",
    "    print(f\"RESULT: {num_coint_relationships} cointegrating relationships exist.\")\n",
    "    print(\"Check the eigenvectors above to see which specific signals are cointegrated.\")\n",
    "else:\n",
    "    print(\"RESULT: No cointegration detected among the signals.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate beta (hedge ratio) using the first cointegrating vector\n",
    "if num_coint_relationships > 0:\n",
    "    # Initialize arrays for rolling calculations\n",
    "    window_size = 252  # One trading year\n",
    "    rolling_spreads = np.zeros((data_matrix.shape[0] - window_size + 1, 1))\n",
    "    rolling_betas = np.zeros((data_matrix.shape[0] - window_size + 1, len(signal_names)-1))\n",
    "    rolling_means = np.zeros(data_matrix.shape[0] - window_size + 1)\n",
    "    rolling_stds = np.zeros(data_matrix.shape[0] - window_size + 1)\n",
    "    \n",
    "    # Calculate rolling values\n",
    "    for i in range(len(rolling_spreads)):\n",
    "        window_data = data_matrix[i:i+window_size]\n",
    "        \n",
    "        # Calculate rolling Johansen test\n",
    "        try:\n",
    "            rolling_johansen = coint_johansen(window_data, det_order=0, k_ar_diff=1)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Handle singular matrix error by skipping this window\n",
    "            rolling_johansen = None\n",
    "            continue\n",
    "        if rolling_johansen is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate rolling betas\n",
    "        rolling_betas[i] = -rolling_johansen.evec[1:, 0] / rolling_johansen.evec[0, 0]\n",
    "        \n",
    "        # Calculate rolling spread\n",
    "        spread = np.zeros(window_size)\n",
    "        for j in range(window_data.shape[1]):\n",
    "            spread += rolling_johansen.evec[j, 0] * window_data[:, j]\n",
    "            \n",
    "        rolling_spreads[i] = spread[-1]  # Store only the last value\n",
    "        rolling_means[i] = np.mean(spread)\n",
    "        rolling_stds[i] = np.std(spread)\n",
    "    \n",
    "    # Plot rolling betas\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for i, name in enumerate(signal_names[1:]):\n",
    "        plt.plot(rolling_betas[:, i], label=f'Beta {name} vs Original', alpha=0.7)\n",
    "    plt.title('Rolling Hedge Ratios (Beta)')\n",
    "    plt.xlabel('Time Window')\n",
    "    plt.ylabel('Beta Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot rolling spread with bands\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(rolling_spreads, label='Spread', alpha=0.7)\n",
    "    plt.plot(rolling_means, label='Rolling Mean', linestyle='--', alpha=0.7)\n",
    "    plt.fill_between(range(len(rolling_spreads)), \n",
    "                     rolling_means - 2*rolling_stds,\n",
    "                     rolling_means + 2*rolling_stds,\n",
    "                     alpha=0.2, label='±2 Std Dev')\n",
    "    plt.title(f'Rolling Cointegration Spread (Window Size: {window_size} periods)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Spread')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final period statistics\n",
    "    print(\"\\nFINAL PERIOD STATISTICS:\")\n",
    "    print(\"Hedge Ratios (Beta):\")\n",
    "    for i, name in enumerate(signal_names[1:]):\n",
    "        print(f\"{name} vs Original: {rolling_betas[-1, i]:.4f}\")\n",
    "    print(f\"\\nSpread Mean: {rolling_means[-1]:.4f}\")\n",
    "    print(f\"Spread Std Dev: {rolling_stds[-1]:.4f}\")\n",
    "    current_zscore = (rolling_spreads[-1] - rolling_means[-1]) / rolling_stds[-1]\n",
    "    print(f\"Current Z-Score: {current_zscore[0]:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo cointegrating relationships found - cannot calculate hedge ratios and spread.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between bidPrice0 of backtest_3_df and backtest_4_df\n",
    "import numpy as np\n",
    "\n",
    "# Ensure both dataframes have the same length by taking the minimum\n",
    "min_length = min(len(backtest_3_df[0][0]), len(backtest_4_df[0][0]))\n",
    "print(min_length)\n",
    "backtest_3_prices = backtest_3_df[0][0]['bidPrice0'][:min_length]\n",
    "backtest_4_prices = backtest_4_df[0][0]['bidPrice0'][:min_length]\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(backtest_3_prices, backtest_4_prices)[0, 1]\n",
    "print(f\"Correlation between bidPrice0 of backtest_3_df and backtest_4_df: {correlation:.6f}\")\n",
    "\n",
    "# Plot the two bidPrices against each other\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot backtest_4_prices on the left y-axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Timestamp Event')\n",
    "ax1.set_ylabel('Backtest 4 bidPrice0 (Exchange 4, Security 1)', color=color)\n",
    "ax1.plot(backtest_3_df[0][0]['timestampEvent'][:min_length], backtest_4_prices, alpha=0.6, linewidth=1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for backtest_3_prices\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Backtest 3 bidPrice0 (Exchange 1, Security 3)', color=color)\n",
    "ax2.plot(backtest_3_df[0][0]['timestampEvent'][:min_length], backtest_3_prices, alpha=0.6, linewidth=1, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.title(f'Correlation between bidPrice0 of Different Exchanges\\nCorrelation: {correlation:.6f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_list = backtest.run(data_type='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MarketDataClient(bucket=\"gnome-market-data-dev\", aws_profile_name=\"AWSAdministratorAccess-443370708724\")\n",
    "client_data_params = {\n",
    "    \"exchange_id\": 1,\n",
    "    \"security_id\": 1,\n",
    "    \"start_datetime\": datetime.datetime(2025, 6, 7),\n",
    "    \"end_datetime\": datetime.datetime(2025, 6, 8),\n",
    "    \"schema_type\": SchemaType.MBP_10,\n",
    "}\n",
    "\n",
    "\n",
    "strategies = [Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_mean_500_delta'], minimum_ticker_cycle=3, starting_cash=100000.0)]\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta_alpha_00005'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta_alpha_0001'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_volatility_breakout'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_mean_reversion'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rsi_strategy'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_macd_strategy'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_moving_average_crossover'], minimum_ticker_cycle=3, starting_cash=100000.0)]\n",
    "# strategies = [Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta'], minimum_ticker_cycle=3, starting_cash=100000.0),]\n",
    "\n",
    "backtest_3 = Backtest(\n",
    "    client=client,\n",
    "    strategies=strategies,  # Pass a list of strategies\n",
    "    exchange_id=client_data_params[\"exchange_id\"],\n",
    "    security_id=client_data_params[\"security_id\"],\n",
    "    start_datetime=client_data_params[\"start_datetime\"],\n",
    "    end_datetime=client_data_params[\"end_datetime\"],\n",
    "    schema_type=client_data_params[\"schema_type\"]\n",
    ")\n",
    "backtest_3_df = backtest_3.run(data_type='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the rolling and exponentially weighted moving average (ewm) volatility of bidPrice0\n",
    "action_name = \"Volatility\"\n",
    "output_df = outputs_list[0][0]\n",
    "\n",
    "# Calculate rolling volatility\n",
    "rolling_volatility = output_df['bidPrice0'].rolling(window=20000).std()\n",
    "\n",
    "# Calculate exponentially weighted moving average volatility\n",
    "ewm_volatility = output_df['bidPrice0'].ewm(span=20000, adjust=False).std()\n",
    "\n",
    "# Classify each row into volatility categories based on ewm volatility\n",
    "volatility_thresholds = ewm_volatility.quantile([0.33, 0.66])\n",
    "output_df['volatility_category'] = pd.cut(\n",
    "    ewm_volatility,\n",
    "    bins=[-float('inf'), volatility_thresholds[0.33], volatility_thresholds[0.66], float('inf')],\n",
    "    labels=['low', 'medium', 'high']\n",
    ")\n",
    "\n",
    "# Plot the rolling and ewm volatility with zones\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(output_df['timestampEvent'], rolling_volatility, label='Rolling 100 Ticker Volatility of Bid Price', color='purple')\n",
    "plt.plot(output_df['timestampEvent'], ewm_volatility, label='EWM Volatility of Bid Price', color='orange')\n",
    "\n",
    "\n",
    "# Highlight the volatility zones\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'low', color='green', alpha=0.3, label='Low Volatility Zone')\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'medium', color='yellow', alpha=0.3, label='Medium Volatility Zone')\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'high', color='red', alpha=0.3, label='High Volatility Zone')\n",
    "\n",
    "plt.xlabel('Timestamp Event')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title(f'Rolling and EWM Volatility of Bid Price for {action_name}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% cell 4.5 code\n",
    "\n",
    "# Plot bidPrice0 below the volatility graph\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(output_df['timestampEvent'], output_df['bidPrice0'], label='Bid Price', color='blue')\n",
    "plt.xlabel('Timestamp Event')\n",
    "plt.ylabel('Bid Price')\n",
    "plt.title('Bid Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration Trading Strategy Explanation\n",
    "\n",
    "print(\"=== COINTEGRATION TRADING STRATEGY EXPLANATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"HIGH-LEVEL OVERVIEW:\")\n",
    "print(\"Cointegration is a statistical relationship between two or more time series that\")\n",
    "print(\"tend to move together over the long term, even though they may diverge in the short term.\")\n",
    "print(\"As a hedge fund manager, this creates pairs trading opportunities where we can profit\")\n",
    "print(\"from temporary divergences while maintaining market-neutral exposure.\")\n",
    "print()\n",
    "\n",
    "print(\"DETAILED EXPLANATION:\")\n",
    "print()\n",
    "\n",
    "print(\"1. CONCEPT:\")\n",
    "print(\"   - Two assets are cointegrated if their price difference is stationary\")\n",
    "print(\"   - Even if individual prices are non-stationary (trending), their spread reverts to mean\")\n",
    "print(\"   - Creates predictable trading opportunities when the spread deviates from equilibrium\")\n",
    "print()\n",
    "\n",
    "print(\"2. MATHEMATICAL FOUNDATION:\")\n",
    "print(\"   - If Price_A and Price_B are cointegrated:\")\n",
    "print(\"   - Spread = Price_A - β × Price_B (where β is the hedge ratio)\")\n",
    "print(\"   - The spread follows a mean-reverting process\")\n",
    "print(\"   - When spread > mean + threshold: Short A, Long B\")\n",
    "print(\"   - When spread < mean - threshold: Long A, Short B\")\n",
    "print()\n",
    "\n",
    "print(\"3. TRADING MECHANISM:\")\n",
    "print(\"   Step 1: Identify cointegrated pairs using statistical tests (Engle-Granger, Johansen)\")\n",
    "print(\"   Step 2: Calculate the hedge ratio (β) using regression or error correction models\")\n",
    "print(\"   Step 3: Monitor the spread = Price_A - β × Price_B\")\n",
    "print(\"   Step 4: Enter positions when spread deviates significantly from mean\")\n",
    "print(\"   Step 5: Exit when spread reverts to mean\")\n",
    "print()\n",
    "\n",
    "print(\"4. HEDGE FUND IMPLEMENTATION:\")\n",
    "print(\"   ADVANTAGES:\")\n",
    "print(\"   - Market-neutral strategy (hedged against market direction)\")\n",
    "print(\"   - Statistical edge based on mean reversion\")\n",
    "print(\"   - Lower volatility than directional strategies\")\n",
    "print(\"   - Scalable across multiple pairs\")\n",
    "print(\"   - Works in various market conditions\")\n",
    "print()\n",
    "print(\"   CHALLENGES:\")\n",
    "print(\"   - Cointegration relationships can break down\")\n",
    "print(\"   - Requires sophisticated statistical analysis\")\n",
    "print(\"   - Transaction costs can erode profits\")\n",
    "print(\"   - Need real-time monitoring of multiple pairs\")\n",
    "print(\"   - Risk of prolonged divergence before convergence\")\n",
    "print()\n",
    "\n",
    "print(\"5. REAL-WORLD EXAMPLE - CURRENCY PAIRS:\")\n",
    "print(\"   Consider EUR/USD and GBP/USD:\")\n",
    "print(\"   - Both influenced by USD strength/weakness\")\n",
    "print(\"   - Economic ties between EUR and GBP create cointegration\")\n",
    "print()\n",
    "print(\"   Historical Analysis:\")\n",
    "print(\"   - EUR/USD = 1.1000, GBP/USD = 1.3000\")\n",
    "print(\"   - Hedge ratio (β) = 0.85 (from regression analysis)\")\n",
    "print(\"   - Normal spread = 1.1000 - 0.85 × 1.3000 = 0.0950\")\n",
    "print()\n",
    "print(\"   Trading Opportunity:\")\n",
    "print(\"   - Current: EUR/USD = 1.0800, GBP/USD = 1.3200\")\n",
    "print(\"   - Current spread = 1.0800 - 0.85 × 1.3200 = -0.0420\")\n",
    "print(\"   - Spread is 0.137 below normal (significant deviation)\")\n",
    "print(\"   - Trade: Long EUR/USD, Short GBP/USD\")\n",
    "print(\"   - Exit when spread returns to ~0.095\")\n",
    "print()\n",
    "\n",
    "print(\"6. STATISTICAL TESTS & IMPLEMENTATION:\")\n",
    "print(\"   A. Engle-Granger Test:\")\n",
    "print(\"      - Regress Price_A on Price_B\")\n",
    "print(\"      - Test residuals for stationarity using ADF test\")\n",
    "print(\"      - If residuals are stationary, pairs are cointegrated\")\n",
    "print()\n",
    "print(\"   B. Johansen Test:\")\n",
    "print(\"      - Tests for multiple cointegrating relationships\")\n",
    "print(\"      - More robust for multiple time series\")\n",
    "print(\"      - Provides cointegrating vectors directly\")\n",
    "print()\n",
    "print(\"   C. Error Correction Model:\")\n",
    "print(\"      - ΔPrice_A = α × (Price_A - β × Price_B)_{t-1} + noise\")\n",
    "print(\"      - α measures speed of mean reversion\")\n",
    "print(\"      - Higher |α| indicates faster convergence\")\n",
    "print()\n",
    "\n",
    "print(\"7. RISK MANAGEMENT:\")\n",
    "print(\"   - Position sizing based on historical volatility of spread\")\n",
    "print(\"   - Stop-loss when spread exceeds historical extremes\")\n",
    "print(\"   - Regular re-estimation of cointegration parameters\")\n",
    "print(\"   - Monitoring for structural breaks in relationships\")\n",
    "print(\"   - Diversification across multiple cointegrated pairs\")\n",
    "print()\n",
    "\n",
    "print(\"8. ADVANCED APPLICATIONS:\")\n",
    "print(\"   A. Multi-Asset Cointegration:\")\n",
    "print(\"      - Basket of currencies vs. single currency\")\n",
    "print(\"      - Sector rotation strategies\")\n",
    "print(\"      - Cross-asset cointegration (bonds vs. stocks)\")\n",
    "print()\n",
    "print(\"   B. Dynamic Hedge Ratios:\")\n",
    "print(\"      - Kalman filters for time-varying relationships\")\n",
    "print(\"      - Rolling window estimation\")\n",
    "print(\"      - Regime-switching models\")\n",
    "print()\n",
    "print(\"   C. High-Frequency Implementation:\")\n",
    "print(\"      - Intraday cointegration patterns\")\n",
    "print(\"      - Tick-by-tick spread monitoring\")\n",
    "print(\"      - Latency arbitrage opportunities\")\n",
    "print()\n",
    "\n",
    "print(\"9. COMPARISON TO TRIANGULAR ARBITRAGE:\")\n",
    "print(\"   TRIANGULAR ARBITRAGE:\")\n",
    "print(\"   - Risk-free, immediate profit\")\n",
    "print(\"   - Requires perfect execution\")\n",
    "print(\"   - Opportunities are rare and fleeting\")\n",
    "print()\n",
    "print(\"   COINTEGRATION TRADING:\")\n",
    "print(\"   - Statistical edge, not risk-free\")\n",
    "print(\"   - Longer holding periods\")\n",
    "print(\"   - More frequent opportunities\")\n",
    "print(\"   - Requires sophisticated modeling\")\n",
    "print()\n",
    "\n",
    "print(\"10. PRACTICAL EXAMPLE - IMPLEMENTATION STEPS:\")\n",
    "print(\"    # Step 1: Data Collection\")\n",
    "print(\"    eur_usd = get_price_data('EUR/USD')\")\n",
    "print(\"    gbp_usd = get_price_data('GBP/USD')\")\n",
    "print()\n",
    "print(\"    # Step 2: Test for Cointegration\")\n",
    "print(\"    from statsmodels.tsa.stattools import coint\")\n",
    "print(\"    score, pvalue, _ = coint(eur_usd, gbp_usd)\")\n",
    "print()\n",
    "print(\"    # Step 3: Calculate Hedge Ratio\")\n",
    "print(\"    hedge_ratio = np.polyfit(gbp_usd, eur_usd, 1)[0]\")\n",
    "print()\n",
    "print(\"    # Step 4: Calculate Spread\")\n",
    "print(\"    spread = eur_usd - hedge_ratio * gbp_usd\")\n",
    "print()\n",
    "print(\"    # Step 5: Generate Signals\")\n",
    "print(\"    z_score = (spread - spread.mean()) / spread.std()\")\n",
    "print(\"    signals = np.where(z_score > 2, -1,  # Short spread\")\n",
    "print(\"                      np.where(z_score < -2, 1, 0))  # Long spread\")\n",
    "print()\n",
    "\n",
    "print(\"As a hedge fund manager, cointegration strategies offer a more sustainable\")\n",
    "print(\"approach than pure arbitrage. While not risk-free, they provide statistical\")\n",
    "print(\"edges that can be systematically exploited with proper risk management,\")\n",
    "print(\"making them a cornerstone of many quantitative hedge fund strategies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = outputs_list[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create the plot for buy/sell signals for each strategy\n",
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    # Sample every 1000th row for the current strategy's output\n",
    "    sampled_df = output_df.iloc[::1000, :]\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=sampled_df, x='timestampEvent', y='bidPrice0', label='Bid Price')\n",
    "    sns.lineplot(data=sampled_df, x='timestampEvent', y='askPrice0', label='Ask Price')\n",
    "    plt.xlabel('Timestamp Event')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Bid and Ask Prices')\n",
    "\n",
    "\n",
    "    # Add green and red dots for actions specific to each strategy\n",
    "    buy_signals = output_df[output_df[f'{action_name}_cash_action'] > 0]\n",
    "    sell_signals = output_df[output_df[f'{action_name}_cash_action'] < 0]\n",
    "\n",
    "    plt.scatter(buy_signals['timestampEvent'], buy_signals['askPrice0'], color='green', label='Buy Signal', marker='o')\n",
    "    plt.scatter(sell_signals['timestampEvent'], sell_signals['askPrice0'], color='red', label='Sell Signal', marker='x')\n",
    "\n",
    "    plt.xlabel('Timestamp Event')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Buy/Sell Signals for {action_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot Equity Position on the first y-axis\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Equity Position', color='tab:blue')\n",
    "    sns.lineplot(data=output_df, x=output_df.index, y=f'{action_name}_equity_position', label='Equity Position', ax=ax1, color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    # Create a second y-axis for Cash Balance\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Cash Balance', color='tab:orange')\n",
    "    sns.lineplot(data=output_df, x=output_df.index, y=f'{action_name}_cash_balance', label='Cash Balance', ax=ax2, color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Add title and legend\n",
    "    fig.suptitle(f'Equity Position and Cash Balance Over Time for {action_name}')\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store metrics for each strategy\n",
    "strategy_metrics = {}\n",
    "\n",
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    # Classify each row into volatility categories based on ewm volatility\n",
    "    ewm_volatility = output_df['bidPrice0'].ewm(span=20000, adjust=False).std()\n",
    "    volatility_thresholds = ewm_volatility.quantile([0.33, 0.66])\n",
    "    output_df['volatility_category'] = pd.cut(\n",
    "        ewm_volatility,\n",
    "        bins=[-float('inf'), volatility_thresholds[0.33], volatility_thresholds[0.66], float('inf')],\n",
    "        labels=['low', 'medium', 'high']\n",
    "    )\n",
    "\n",
    "    # Initialize a dictionary to store metrics for each volatility zone\n",
    "    strategy_metrics[action_name] = {}\n",
    "\n",
    "    # Calculate and store advanced HFT metrics for each volatility zone\n",
    "    for volatility_zone in ['low', 'medium', 'high']:\n",
    "        zone_df = output_df[output_df['volatility_category'] == volatility_zone]\n",
    "\n",
    "        # Identify sectors within the volatility zone\n",
    "        zone_df['sector'] = (zone_df.index.to_series().diff() != 1).cumsum()\n",
    "\n",
    "        # Calculate metrics at the sector level\n",
    "        sector_metrics = []\n",
    "        for sector, sector_df in zone_df.groupby('sector'):\n",
    "            mean_bid_price = sector_df['bidPrice0'].mean()\n",
    "            mean_ask_price = sector_df['askPrice0'].mean()\n",
    "            total_trades = len(sector_df[sector_df[f'{action_name}_cash_action'] != 0])\n",
    "            profit_factor = (sector_df[f'{action_name}_cash_balance'].iloc[-1] - strategy.starting_cash) / strategy.starting_cash if strategy.starting_cash != 0 else float('inf')\n",
    "            number_of_buys = len(sector_df[sector_df[f'{action_name}_cash_action'] > 0])\n",
    "            number_of_sells = len(sector_df[sector_df[f'{action_name}_cash_action'] < 0])\n",
    "\n",
    "            sector_metrics.append({\n",
    "                'mean_bid_price': mean_bid_price,\n",
    "                'mean_ask_price': mean_ask_price,\n",
    "                'total_trades': total_trades,\n",
    "                'profit_factor': profit_factor,\n",
    "                'number_of_buys': number_of_buys,\n",
    "                'number_of_sells': number_of_sells\n",
    "            })\n",
    "        \n",
    "    \n",
    "        # Calculate mean and std of the sector metrics at the volatility level\n",
    "        mean_metrics = {key: np.mean([sector[key] for sector in sector_metrics]) for key in sector_metrics[0]}\n",
    "        std_metrics = {key: np.std([sector[key] for sector in sector_metrics]) for key in sector_metrics[0]}\n",
    "\n",
    "        # Store the metrics for each strategy and volatility zone\n",
    "        strategy_metrics[action_name][volatility_zone] = {\n",
    "            'mean_metrics': mean_metrics,\n",
    "            'std_metrics': std_metrics,\n",
    "            \"all_metrics\": sector_metrics\n",
    "        }\n",
    "\n",
    "# Now strategy_metrics contains all the metrics for each strategy and volatility zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from strategy_metrics for each strategy and volatility zone\n",
    "all_metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': strategy,\n",
    "        'Volatility': volatility_zone,\n",
    "        'Profit Factor': metric['profit_factor']\n",
    "    }\n",
    "    for strategy in strategy_metrics\n",
    "    for volatility_zone in strategy_metrics[strategy]\n",
    "    for metric in strategy_metrics[strategy][volatility_zone]['all_metrics']\n",
    "])\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the DataFrame using the mean and std metrics\n",
    "profit_factors_df = all_metrics_df\n",
    "\n",
    "# Set the theme for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the figure with increased height to space strategies more\n",
    "f, ax = plt.subplots(figsize=(8, 12))  # Adjust the height to space out the strategies\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Define a custom palette for volatility levels\n",
    "custom_palette = {'low': 'green', 'medium': 'yellow', 'high': 'red'}\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(\n",
    "    data=profit_factors_df, x=\"Profit Factor\", y=\"Strategy\", hue=\"Volatility\",\n",
    "    dodge=True, alpha=.25, zorder=1, palette=custom_palette\n",
    ")\n",
    "\n",
    "# Add mean and std as black font labels next to each stripplot for each volatility zone\n",
    "for i, (strategy, strategy_df) in enumerate(profit_factors_df.groupby('Strategy')):\n",
    "    # Ensure the subplot's stripplot strategy matches the strategy of the mean being printed\n",
    "    strategy_index = profit_factors_df['Strategy'].unique().tolist().index(strategy)\n",
    "    for j, (volatility, volatility_df) in enumerate(strategy_df.groupby('Volatility')):\n",
    "        strategy_volatility_data = volatility_df['Profit Factor']\n",
    "        mean = strategy_volatility_data.mean()\n",
    "        std = strategy_volatility_data.std()\n",
    "        # Align the text horizontally with the respective volatility zones\n",
    "        ax.text(2.9, strategy_index + j * 0.2 - 0.2, f'Mean: {mean:.2f} +/- {std:.2f}', color='black', ha='right', va='top', fontsize='small')\n",
    "\n",
    "# Add a bold black vertical line at 1\n",
    "ax.axvline(x=1, color='black', linewidth=2, linestyle='-')\n",
    "\n",
    "# Set the x-axis limits from 0 to 2\n",
    "ax.set_xlim(-1, 3)\n",
    "\n",
    "# Add horizontal lines to separate each row\n",
    "for ytick in range(len(ax.get_yticks()) + 1):\n",
    "    ax.axhline(y=ytick - 0.5, color='black', linewidth=0.5, linestyle='-')\n",
    "\n",
    "# Add the legend manually since move_legend requires an existing legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, loc=\"lower right\", ncol=3, frameon=True, columnspacing=1, handletextpad=0, fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
