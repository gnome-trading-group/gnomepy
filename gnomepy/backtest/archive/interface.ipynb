{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnomepy.backtest.backtest import *\n",
    "from gnomepy.backtest.strategy import *\n",
    "from gnomepy.backtest.archive.signal import *\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from gnomepy.backtest.archive.coint_testing import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Cointegration Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Data from Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exchangeId</th>\n",
       "      <th>securityId</th>\n",
       "      <th>timestampEvent</th>\n",
       "      <th>timestampSent</th>\n",
       "      <th>timestampRecv</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>action</th>\n",
       "      <th>side</th>\n",
       "      <th>flags</th>\n",
       "      <th>...</th>\n",
       "      <th>bidSize8</th>\n",
       "      <th>askSize8</th>\n",
       "      <th>bidCount8</th>\n",
       "      <th>askCount8</th>\n",
       "      <th>bidPrice9</th>\n",
       "      <th>askPrice9</th>\n",
       "      <th>bidSize9</th>\n",
       "      <th>askSize9</th>\n",
       "      <th>bidCount9</th>\n",
       "      <th>askCount9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-06 23:59:59.903000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06-07 00:00:00.174904134+00:00</td>\n",
       "      <td>104245.0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>b'M'</td>\n",
       "      <td>b'N'</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47863</td>\n",
       "      <td>0.06011</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>104235.0</td>\n",
       "      <td>104254.0</td>\n",
       "      <td>0.43232</td>\n",
       "      <td>0.09730</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-07 00:00:00.692000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06-07 00:00:03.750969246+00:00</td>\n",
       "      <td>104245.0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>b'M'</td>\n",
       "      <td>b'N'</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47863</td>\n",
       "      <td>0.06011</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>104235.0</td>\n",
       "      <td>104254.0</td>\n",
       "      <td>0.29060</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-07 00:00:00.786000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06-07 00:00:03.764506626+00:00</td>\n",
       "      <td>104245.0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>b'T'</td>\n",
       "      <td>b'B'</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47863</td>\n",
       "      <td>0.06011</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>104235.0</td>\n",
       "      <td>104254.0</td>\n",
       "      <td>0.29060</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-07 00:00:01.321000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06-07 00:00:03.800315444+00:00</td>\n",
       "      <td>104245.0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>b'M'</td>\n",
       "      <td>b'N'</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47863</td>\n",
       "      <td>0.06011</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>104235.0</td>\n",
       "      <td>104254.0</td>\n",
       "      <td>0.28810</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-07 00:00:02.974000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06-07 00:00:03.840272754+00:00</td>\n",
       "      <td>104245.0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>b'M'</td>\n",
       "      <td>b'N'</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47863</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>104235.0</td>\n",
       "      <td>104254.0</td>\n",
       "      <td>0.28810</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   exchangeId  securityId                   timestampEvent timestampSent  \\\n",
       "0           1           1 2025-06-06 23:59:59.903000+00:00           NaT   \n",
       "1           1           1 2025-06-07 00:00:00.692000+00:00           NaT   \n",
       "2           1           1 2025-06-07 00:00:00.786000+00:00           NaT   \n",
       "3           1           1 2025-06-07 00:00:01.321000+00:00           NaT   \n",
       "4           1           1 2025-06-07 00:00:02.974000+00:00           NaT   \n",
       "\n",
       "                        timestampRecv     price    size action  side  flags  \\\n",
       "0 2025-06-07 00:00:00.174904134+00:00  104245.0  0.0008   b'M'  b'N'     16   \n",
       "1 2025-06-07 00:00:03.750969246+00:00  104245.0  0.0008   b'M'  b'N'     16   \n",
       "2 2025-06-07 00:00:03.764506626+00:00  104245.0  0.0008   b'T'  b'B'     16   \n",
       "3 2025-06-07 00:00:03.800315444+00:00  104245.0  0.0008   b'M'  b'N'     16   \n",
       "4 2025-06-07 00:00:03.840272754+00:00  104245.0  0.0008   b'M'  b'N'     16   \n",
       "\n",
       "   ...  bidSize8  askSize8  bidCount8  askCount8  bidPrice9  askPrice9  \\\n",
       "0  ...   0.47863   0.06011          2          2   104235.0   104254.0   \n",
       "1  ...   0.47863   0.06011          2          2   104235.0   104254.0   \n",
       "2  ...   0.47863   0.06011          2          2   104235.0   104254.0   \n",
       "3  ...   0.47863   0.06011          2          2   104235.0   104254.0   \n",
       "4  ...   0.47863   0.00011          2          1   104235.0   104254.0   \n",
       "\n",
       "   bidSize9  askSize9  bidCount9  askCount9  \n",
       "0   0.43232   0.09730          7          2  \n",
       "1   0.29060   0.00011          5          1  \n",
       "2   0.29060   0.00011          5          1  \n",
       "3   0.28810   0.00011          4          1  \n",
       "4   0.28810   0.00011          4          1  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MarketDataClient(bucket=\"gnome-market-data-prod\", aws_profile_name=\"AWSAdministratorAccess-241533121172\")\n",
    "client_data_params = {\n",
    "    \"exchange_id\": 1,\n",
    "    \"security_id\": 1,\n",
    "    \"start_datetime\": datetime.datetime(2025, 6, 7),\n",
    "    \"end_datetime\": datetime.datetime(2025, 6, 8),\n",
    "    \"schema_type\": SchemaType.MBP_10,\n",
    "}\n",
    "data = client.get_data(**client_data_params).to_df()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manufacture Synthetic Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:44: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data[f'bidPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_bid_price\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'bidPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_bid_price\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:45: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data[f'askPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_ask_price\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'askPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_ask_price\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:50: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data[f'bidPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_bid\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'bidPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_bid\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:51: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data[f'askPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_ask\n",
      "/var/folders/xs/59kj_n6d2qs33ypnrkskvyzh0000gn/T/ipykernel_32836/1114306985.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'askPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_ask\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set using a slice indexer with a different length than the value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m lagged_bid_price = data[\u001b[33m'\u001b[39m\u001b[33mbidPrice0\u001b[39m\u001b[33m'\u001b[39m].iloc[start_idx-lag:start_idx-lag+lag].values\n\u001b[32m     43\u001b[39m lagged_ask_price = lagged_bid_price + avg_spread\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbidPrice0_perturbed_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m+\u001b[49m\u001b[43mlag\u001b[49m\u001b[43m]\u001b[49m = lagged_bid_price\n\u001b[32m     45\u001b[39m data[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maskPrice0_perturbed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m].iloc[start_idx:start_idx+lag] = lagged_ask_price\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# --- For bidPrice0_random_normal perturbed series (bid and ask) ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:911\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    910\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1944\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1942\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_with_indexer_split_path(indexer, value, name)\n\u001b[32m   1943\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1944\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_single_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:2218\u001b[39m, in \u001b[36m_iLocIndexer._setitem_single_block\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   2215\u001b[39m \u001b[38;5;28mself\u001b[39m.obj._check_is_chained_assignment_possible()\n\u001b[32m   2217\u001b[39m \u001b[38;5;66;03m# actually do the set\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2218\u001b[39m \u001b[38;5;28mself\u001b[39m.obj._mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[38;5;28mself\u001b[39m.obj._maybe_update_cacher(clear=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:415\u001b[39m, in \u001b[36mBaseBlockManager.setitem\u001b[39m\u001b[34m(self, indexer, value, warn)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msetitem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:1403\u001b[39m, in \u001b[36mBlock.setitem\u001b[39m\u001b[34m(self, indexer, value, using_cow)\u001b[39m\n\u001b[32m   1400\u001b[39m     values = values.T\n\u001b[32m   1402\u001b[39m \u001b[38;5;66;03m# length checking\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m \u001b[43mcheck_setitem_lengths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype != _dtype_obj:\n\u001b[32m   1406\u001b[39m     \u001b[38;5;66;03m# GH48933: extract_array would convert a pd.Series value to np.ndarray\u001b[39;00m\n\u001b[32m   1407\u001b[39m     value = extract_array(value, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/pandas/core/indexers/utils.py:177\u001b[39m, in \u001b[36mcheck_setitem_lengths\u001b[39m\u001b[34m(indexer, value, values)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) != length_of_indexer(indexer, values) \u001b[38;5;129;01mand\u001b[39;00m values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    176\u001b[39m         \u001b[38;5;66;03m# In case of two dimensional value is used row-wise and broadcasted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    178\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcannot set using a slice indexer with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdifferent length than the value\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         )\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[32m    182\u001b[39m         no_op = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot set using a slice indexer with a different length than the value"
     ]
    }
   ],
   "source": [
    "# Calculate average spread between ask and bid\n",
    "avg_spread = (data['askPrice0'] - data['bidPrice0']).mean()\n",
    "\n",
    "# Create lagged signals with different lag lengths\n",
    "lag_lengths = [10, 200, 600]  # Short, medium and long lags\n",
    "n = len(data)\n",
    "\n",
    "# Initialize perturbed series with original prices (bidPrice0)\n",
    "data['bidPrice0_perturbed_1'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "data['bidPrice0_perturbed_2'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "data['bidPrice0_perturbed_3'] = data['bidPrice0'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0']/10)\n",
    "\n",
    "# Initialize perturbed ask prices by adding spread to perturbed bid prices\n",
    "data['askPrice0_perturbed_1'] = data['bidPrice0_perturbed_1'] + avg_spread\n",
    "data['askPrice0_perturbed_2'] = data['bidPrice0_perturbed_2'] + avg_spread\n",
    "data['askPrice0_perturbed_3'] = data['bidPrice0_perturbed_3'] + avg_spread\n",
    "\n",
    "# --- New: Create random normal walk and its perturbed versions ---\n",
    "# Create a random walk series\n",
    "data['bidPrice0_random_normal'] = np.cumsum(np.random.normal(size=len(data))) + 10000\n",
    "data['askPrice0_random_normal'] = data['bidPrice0_random_normal'] + avg_spread\n",
    "\n",
    "# Initialize perturbed versions of the random walk\n",
    "data['bidPrice0_random_normal_perturbed_1'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "data['bidPrice0_random_normal_perturbed_2'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "data['bidPrice0_random_normal_perturbed_3'] = data['bidPrice0_random_normal'] + np.random.normal(loc=0, scale=0.01, size=len(data)) * (data['bidPrice0_random_normal']/10)\n",
    "\n",
    "# Initialize random_normal_perturbed ask prices by adding spread to random_normal_perturbed bid prices\n",
    "data['askPrice0_random_normal_perturbed_1'] = data['bidPrice0_random_normal_perturbed_1'] + avg_spread\n",
    "data['askPrice0_random_normal_perturbed_2'] = data['bidPrice0_random_normal_perturbed_2'] + avg_spread\n",
    "data['askPrice0_random_normal_perturbed_3'] = data['bidPrice0_random_normal_perturbed_3'] + avg_spread\n",
    "\n",
    "# Generate random lag points for each series\n",
    "num_lags = n // 1000  # Create lags roughly every 1000 ticks\n",
    "lag_points = np.sort(np.random.choice(range(n-max(lag_lengths)), num_lags, replace=False))\n",
    "\n",
    "# Apply lags at random points for both bidPrice0 and bidPrice0_random_normal perturbed series\n",
    "for start_idx in lag_points:\n",
    "    # For each lag length (short, medium, long)\n",
    "    for i, lag in enumerate(lag_lengths, 1):\n",
    "        # --- For bidPrice0 perturbed series ---\n",
    "        lagged_bid_price = data['bidPrice0'].iloc[start_idx-lag:start_idx-lag+lag].values\n",
    "        lagged_ask_price = lagged_bid_price + avg_spread\n",
    "        data[f'bidPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_bid_price\n",
    "        data[f'askPrice0_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_ask_price\n",
    "\n",
    "        # --- For bidPrice0_random_normal perturbed series (bid and ask) ---\n",
    "        lagged_random_normal_bid = data['bidPrice0_random_normal'].iloc[start_idx-lag:start_idx-lag+lag].values\n",
    "        lagged_random_normal_ask = lagged_random_normal_bid + avg_spread\n",
    "        data[f'bidPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_bid\n",
    "        data[f'askPrice0_random_normal_perturbed_{i}'].iloc[start_idx:start_idx+lag] = lagged_random_normal_ask\n",
    "\n",
    "# Calculate log of prices\n",
    "data['log_price'] = np.log(data['bidPrice0'])\n",
    "data['log_price_perturbed_1'] = np.log(data['bidPrice0_perturbed_1'])\n",
    "data['log_price_perturbed_2'] = np.log(data['bidPrice0_perturbed_2'])\n",
    "data['log_price_perturbed_3'] = np.log(data['bidPrice0_perturbed_3'])\n",
    "\n",
    "# Also calculate log of random normal and its perturbed versions\n",
    "data['log_price_random_normal'] = np.log(data['bidPrice0_random_normal'])\n",
    "data['log_price_random_normal_perturbed_1'] = np.log(data['bidPrice0_random_normal_perturbed_1'])\n",
    "data['log_price_random_normal_perturbed_2'] = np.log(data['bidPrice0_random_normal_perturbed_2'])\n",
    "data['log_price_random_normal_perturbed_3'] = np.log(data['bidPrice0_random_normal_perturbed_3'])\n",
    "\n",
    "# Drop NaN values from all columns used in plotting\n",
    "data = data.dropna(subset=[\n",
    "    'log_price', 'log_price_perturbed_1', 'log_price_perturbed_2', 'log_price_perturbed_3',\n",
    "    'log_price_random_normal', 'log_price_random_normal_perturbed_1', 'log_price_random_normal_perturbed_2', 'log_price_random_normal_perturbed_3'\n",
    "], axis=0)\n",
    "\n",
    "# Display the log prices, plotting every 1000th point\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price'][::1000], label='Original Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_1'][::1000], label='Perturbed 1% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_2'][::1000], label='Perturbed 2% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_perturbed_3'][::1000], label='Perturbed 3% Log Price', alpha=0.8)\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal'][::1000], label='Random Normal Log Price', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_1'][::1000], label='Random Normal Perturbed 1', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_2'][::1000], label='Random Normal Perturbed 2', alpha=0.8, linestyle='--')\n",
    "# plt.plot(data['timestampEvent'][::1000], data['log_price_random_normal_perturbed_3'][::1000], label='Random Normal Perturbed 3', alpha=0.8, linestyle='--')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Log Price')\n",
    "# plt.title('Log of Original, Perturbed, and Random Normal bidPrice0')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Cointegrated Baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank from test for basket ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_2', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2', 'bidPrice0_random_normal_perturbed_3'): 6\n",
      "Rank from test for basket ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2', 'bidPrice0_random_normal_perturbed_3'): 5\n",
      "Added cointegrated basket: ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2', 'bidPrice0_random_normal_perturbed_3') with 5 eigenvector(s)\n",
      "Rank from test for basket ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_2', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2'): 5\n",
      "Rank from test for basket ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2'): 4\n",
      "Added cointegrated basket: ('bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_3', 'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2') with 4 eigenvector(s)\n"
     ]
    }
   ],
   "source": [
    "seen_baskets, cointegrated_baskets = get_coint_baskets(\n",
    "    columns=[\n",
    "        'bidPrice0', 'bidPrice0_perturbed_1', 'bidPrice0_perturbed_2', 'bidPrice0_perturbed_3', \n",
    "        'bidPrice0_random_normal', 'bidPrice0_random_normal_perturbed_1', 'bidPrice0_random_normal_perturbed_2',\n",
    "        'bidPrice0_random_normal_perturbed_3'\n",
    "    ], \n",
    "    data=data, significance_level=0.10, min_basket_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2 parameter combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'main.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m baskets = \u001b[38;5;28mlist\u001b[39m(cointegrated_baskets.keys())\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Your setup code here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaskets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaskets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_refresh_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspread_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcash_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnotional\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrade_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43menter_zscore\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexit_zscore\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m backtest_results = {basket: {\u001b[33m'\u001b[39m\u001b[33mhistory_df\u001b[39m\u001b[33m'\u001b[39m: history_df, \u001b[33m'\u001b[39m\u001b[33mtrade_log\u001b[39m\u001b[33m'\u001b[39m: trade_log}\n\u001b[32m     19\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m basket, history_df, trade_log \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/gnomepy/backtest/archive/coint_testing.py:650\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(baskets, data, beta_refresh_freq, spread_window, cash_start, notional, trade_freq, execution_delay, enter_zscore, exit_zscore, stop_loss_delta, retest_cointegration, use_extends, use_lob, use_multiprocessing)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_multiprocessing:\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m mp.Pool(processes=\u001b[32m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m         results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_backtest_for_basket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_combos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    652\u001b[39m     results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gnomepy/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:540\u001b[39m, in \u001b[36mPool._handle_tasks\u001b[39m\u001b[34m(taskqueue, put, outqueue, pool, cache)\u001b[39m\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    542\u001b[39m     job, idx = task[:\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:206\u001b[39m, in \u001b[36m_ConnectionBase.send\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._check_writable()\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28mself\u001b[39m._send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/reduction.py:51\u001b[39m, in \u001b[36mForkingPickler.dumps\u001b[39m\u001b[34m(cls, obj, protocol)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     50\u001b[39m     buf = io.BytesIO()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buf.getbuffer()\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'main.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# Get basksets from cointegration testing\n",
    "baskets = list(cointegrated_baskets.keys())\n",
    "\n",
    "# Your setup code here\n",
    "results = main(\n",
    "    baskets=baskets,\n",
    "    data=data,\n",
    "    beta_refresh_freq=[1000],\n",
    "    spread_window=[100],\n",
    "    cash_start=[10000],\n",
    "    notional=[100],\n",
    "    trade_freq=[1],\n",
    "    execution_delay=[0],\n",
    "    enter_zscore=[2.5],\n",
    "    exit_zscore=[0.1],\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "backtest_results = {basket: {'history_df': history_df, 'trade_log': trade_log}\n",
    "                    for basket, history_df, trade_log in results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_backtest_summary(history_df, trade_log):\n",
    "    import numpy as np\n",
    "\n",
    "    # Compute average profit per complete trade and win ratio\n",
    "    # We assume trade_log has columns: 'action', 'before_cash', 'after_cash', etc.\n",
    "    # We'll look for pairs of (enter_long/enter_short) followed by (exit_long/exit_short)\n",
    "    complete_trade_profits = []\n",
    "    complete_trade_durations = []\n",
    "    open_trade = None\n",
    "    open_trade_idx = None\n",
    "\n",
    "    for idx, row in trade_log.iterrows():\n",
    "        action = row['action']\n",
    "        if action in ['enter_long', 'enter_short']:\n",
    "            # Start of a new trade\n",
    "            open_trade = row\n",
    "            open_trade_idx = idx\n",
    "        elif action in ['exit_long', 'exit_short'] and open_trade is not None:\n",
    "            # End of a trade, compute profit and duration\n",
    "            profit = row['after_cash'] - open_trade['before_cash']\n",
    "            complete_trade_profits.append(profit)\n",
    "            # Calculate ticks between entry and exit (assume index is ordered)\n",
    "            duration = row['ticks_since_entry']\n",
    "            complete_trade_durations.append(duration)\n",
    "            open_trade = None  # Reset for next trade\n",
    "            open_trade_idx = None\n",
    "\n",
    "    if complete_trade_profits:\n",
    "        avg_profit_per_trade = np.round(np.mean(complete_trade_profits), 3)\n",
    "        std_profit_per_trade = np.round(np.std(complete_trade_profits), 3)\n",
    "        num_wins = np.sum(np.array(complete_trade_profits) > 0)\n",
    "        win_ratio = np.round(num_wins / len(complete_trade_profits), 3)\n",
    "        total_profit = np.round(np.sum(complete_trade_profits), 3)\n",
    "        # Profit Factor: sum of profits over sum of losses (absolute value)\n",
    "        profits = np.array(complete_trade_profits)\n",
    "        gross_profit = np.round(profits[profits > 0].sum(), 3)\n",
    "        gross_loss = np.round(-profits[profits < 0].sum(), 3)  # make positive\n",
    "        if gross_loss > 0:\n",
    "            profit_factor = np.round(gross_profit / gross_loss, 3)\n",
    "        else:\n",
    "            profit_factor = np.nan if gross_profit == 0 else np.inf\n",
    "    else:\n",
    "        avg_profit_per_trade = np.nan  # No complete trades\n",
    "        std_profit_per_trade = np.nan\n",
    "        win_ratio = np.nan\n",
    "        total_profit = np.nan\n",
    "        profit_factor = np.nan\n",
    "\n",
    "    if complete_trade_durations:\n",
    "        avg_ticks_per_trade = np.round(np.mean(complete_trade_durations), 3)\n",
    "        std_ticks_per_trade = np.round(np.std(complete_trade_durations), 3)\n",
    "    else:\n",
    "        avg_ticks_per_trade = np.nan\n",
    "        std_ticks_per_trade = np.nan\n",
    "\n",
    "    # --- Max Drawdown Calculation ---\n",
    "    # For max drawdown, always use 'after_cash' from trade_log\n",
    "    max_drawdown = np.nan\n",
    "    if trade_log is not None and not trade_log.empty and 'after_cash' in trade_log.columns:\n",
    "        values = trade_log['after_cash'].values\n",
    "        running_max = np.maximum.accumulate(values)\n",
    "        drawdowns = (values - running_max) / running_max\n",
    "        max_drawdown = np.round(np.min(drawdowns), 3)\n",
    "\n",
    "    summary = {\n",
    "        'num_complete_trades': len(complete_trade_profits),\n",
    "        'avg_profit_per_complete_trade': avg_profit_per_trade,\n",
    "        'std_profit_per_complete_trade': std_profit_per_trade,\n",
    "        'total_profit': total_profit,\n",
    "        'avg_ticks_per_complete_trade': avg_ticks_per_trade,\n",
    "        'std_ticks_per_complete_trade': std_ticks_per_trade,\n",
    "        'win_ratio': win_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'profit_factor': profit_factor\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Generate summary statistics for each basket\n",
    "backtest_summaries = {}\n",
    "for basket, result in backtest_results.items():\n",
    "    history_df = result['history_df']\n",
    "    trade_log = result['trade_log']\n",
    "    summary = compute_backtest_summary(history_df, trade_log)\n",
    "    backtest_summaries[str(basket)] = summary\n",
    "\n",
    "# Optionally, display as DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame.from_dict(backtest_summaries, orient='index')\n",
    "\n",
    "# Round all float columns to 3 decimals for display\n",
    "float_cols = summary_df.select_dtypes(include=['float', 'float64']).columns\n",
    "summary_df[float_cols] = summary_df[float_cols].round(3)\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_history = np.array([[float(obj) if type(obj) is not int and isinstance(obj, (np.matrix, np.float64)) else obj for obj in element] for element in history])\n",
    "new_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(new_history[:,0], new_history[:,1])\n",
    "# plt.plot(new_history[:,0], new_history[:,2])\n",
    "# plt.plot(new_history[:,0], new_history[:,3])\n",
    "plt.plot(new_history[:,0], new_history[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(new_history[:100,0], new_history[:100,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_basket_matrix = np.column_stack([data['log_price'].values[::100], data['log_price_perturbed_1'].values[::100], data['log_price_perturbed_2'].values[::100], data['log_price_perturbed_3'].values[::100]])\n",
    "# Calculate correlation matrix for the log prices\n",
    "corr_matrix = np.corrcoef(coin_basket_matrix.T)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "corr_df = pd.DataFrame(\n",
    "    corr_matrix, \n",
    "    columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'],\n",
    "    index=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3']\n",
    ")\n",
    "\n",
    "print(\"\\nCorrelation Matrix of Log Prices:\")\n",
    "print(corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# Create dataframe of all the relevant coins we would like to run johansen test on\n",
    "coin_basket_matrix = np.column_stack([data['log_price'].values, data['log_price_perturbed_1'].values, data['log_price_perturbed_2'].values, data['log_price_perturbed_3'].values])\n",
    "coin_basket_df = pd.DataFrame(coin_basket_matrix, columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'])\n",
    "\n",
    "# Run johansen test\n",
    "johansen_result = coint_johansen(coin_basket_matrix, det_order=0, k_ar_diff=1)\n",
    "\n",
    "# Compare trace statistics against critical values\n",
    "# If trace stat > critical value, reject null hypothesis and conclude more cointegrating relationships\n",
    "trace_stats = johansen_result.lr1\n",
    "cv_95 = johansen_result.cvt[:, 1]  # 95% critical values\n",
    "\n",
    "num_coints = 0\n",
    "for i in range(len(trace_stats)):\n",
    "    if trace_stats[i] > cv_95[i]:\n",
    "        num_coints += 1\n",
    "\n",
    "beta_vectors = johansen_result.evec[:, :num_coints]\n",
    "beta_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cointegrated series\n",
    "spread = coin_basket_matrix @ beta_vectors\n",
    "z_score = (spread - spread.mean()) / spread.std()\n",
    "\n",
    "# Plot the cointegrated series\n",
    "plt.figure(figsize=(14, 7))\n",
    "# for i in range(cointegrated_series.shape[1]):\n",
    "plt.plot(spread[::4000, 0], label=f'Cointegrated Series {i+1}')\n",
    "plt.plot(z_score[::4000, 0], label=f'ZScore {i+1}')\n",
    "plt.title('Cointegrated Price Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cointegration of the perturbed signals using Johansen cointegration test:\n",
    "\n",
    "print(\"COINTEGRATION ANALYSIS OF PERTURBED SIGNALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import required libraries for cointegration tests\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Get the price series for cointegration testing\n",
    "original_prices = backtest_3_df[0][0]['bidPrice0'].values[:100000]\n",
    "perturbed_1 = backtest_3_df[0][0]['bidPrice0_perturbed_1'].values[:100000]\n",
    "perturbed_2 = backtest_3_df[0][0]['bidPrice0_perturbed_2'].values[:100000]\n",
    "perturbed_3 = backtest_3_df[0][0]['bidPrice0_perturbed_3'].values[:100000]\n",
    "\n",
    "print(\"\\n1. JOHANSEN COINTEGRATION TEST:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Measure timing for Johansen test\n",
    "johansen_start_time = time.time()\n",
    "\n",
    "# Prepare data for Johansen test (all series together)\n",
    "# We have 4 vectors because we're testing 4 time series:\n",
    "# 1. Original bidPrice0 (the baseline price series)\n",
    "# 2. Perturbed_1 (original + 0.05% noise)\n",
    "# 3. Perturbed_2 (original + 10% noise) \n",
    "# 4. Perturbed_3 (original + 15% noise)\n",
    "# Each vector represents one price series, and we want to test if they move together long-term\n",
    "data_matrix = np.column_stack([original_prices, perturbed_1, perturbed_2, perturbed_3])\n",
    "data_df = pd.DataFrame(data_matrix, columns=['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3'])\n",
    "\n",
    "print(f\"Testing cointegration among {data_matrix.shape[1]} price series (vectors):\")\n",
    "print(f\"  - Original bidPrice0\")\n",
    "print(f\"  - Perturbed version 1 (0.05% noise)\")\n",
    "print(f\"  - Perturbed version 2 (10% noise)\")\n",
    "print(f\"  - Perturbed version 3 (15% noise)\")\n",
    "print(f\"Each series has {data_matrix.shape[0]} observations\\n\")\n",
    "\n",
    "# Perform Johansen test\n",
    "johansen_result = coint_johansen(data_matrix, det_order=0, k_ar_diff=1)\n",
    "\n",
    "johansen_end_time = time.time()\n",
    "johansen_duration = johansen_end_time - johansen_start_time\n",
    "\n",
    "# Interpret Johansen test results\n",
    "print(f\"Test completed in {johansen_duration:.3f} seconds\\n\")\n",
    "\n",
    "# Check for cointegration at 5% significance level\n",
    "trace_stats = johansen_result.lr1\n",
    "critical_values_5pct = johansen_result.cvt[:, 1]  # 5% critical values\n",
    "\n",
    "print(\"JOHANSEN TEST RESULTS:\")\n",
    "print(\"Trace Statistics vs Critical Values (5% significance):\")\n",
    "for i in range(len(trace_stats)):\n",
    "    is_cointegrated = trace_stats[i] > critical_values_5pct[i]\n",
    "    print(f\"  H{i}: {trace_stats[i]:.2f} vs {critical_values_5pct[i]:.2f} - {'COINTEGRATED' if is_cointegrated else 'NOT COINTEGRATED'}\")\n",
    "\n",
    "# Count cointegrating relationships\n",
    "num_coint_relationships = sum(trace_stats > critical_values_5pct)\n",
    "\n",
    "print(f\"\\nCOINTEGRATING VECTORS (Eigenvectors):\")\n",
    "print(\"Note: Each eigenvector shows the linear combination weights for the 4 price series\")\n",
    "signal_names = ['Original', 'Perturbed_1', 'Perturbed_2', 'Perturbed_3']\n",
    "for i in range(num_coint_relationships):\n",
    "    print(f\"\\nCointegrating Relationship {i+1}:\")\n",
    "    eigenvector = johansen_result.evec[:, i]\n",
    "    for j, name in enumerate(signal_names):\n",
    "        print(f\"  {name}: {eigenvector[j]:.4f}\")\n",
    "    \n",
    "    # Identify which signals are most strongly related in this relationship\n",
    "    abs_weights = np.abs(eigenvector)\n",
    "    dominant_signals = [signal_names[k] for k in np.where(abs_weights > 0.1)[0]]\n",
    "    print(f\"  Dominant signals in this relationship: {', '.join(dominant_signals)}\")\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Number of cointegrating relationships found: {num_coint_relationships}\")\n",
    "\n",
    "if num_coint_relationships == 4:\n",
    "    print(\"RESULT: All signals are cointegrated - they form a complete cointegrated system.\")\n",
    "    print(\"This means all perturbed versions maintain long-term equilibrium with the original.\")\n",
    "elif num_coint_relationships > 0:\n",
    "    print(f\"RESULT: {num_coint_relationships} cointegrating relationships exist.\")\n",
    "    print(\"Check the eigenvectors above to see which specific signals are cointegrated.\")\n",
    "else:\n",
    "    print(\"RESULT: No cointegration detected among the signals.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate beta (hedge ratio) using the first cointegrating vector\n",
    "if num_coint_relationships > 0:\n",
    "    # Initialize arrays for rolling calculations\n",
    "    window_size = 252  # One trading year\n",
    "    rolling_spreads = np.zeros((data_matrix.shape[0] - window_size + 1, 1))\n",
    "    rolling_betas = np.zeros((data_matrix.shape[0] - window_size + 1, len(signal_names)-1))\n",
    "    rolling_means = np.zeros(data_matrix.shape[0] - window_size + 1)\n",
    "    rolling_stds = np.zeros(data_matrix.shape[0] - window_size + 1)\n",
    "    \n",
    "    # Calculate rolling values\n",
    "    for i in range(len(rolling_spreads)):\n",
    "        window_data = data_matrix[i:i+window_size]\n",
    "        \n",
    "        # Calculate rolling Johansen test\n",
    "        try:\n",
    "            rolling_johansen = coint_johansen(window_data, det_order=0, k_ar_diff=1)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Handle singular matrix error by skipping this window\n",
    "            rolling_johansen = None\n",
    "            continue\n",
    "        if rolling_johansen is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate rolling betas\n",
    "        rolling_betas[i] = -rolling_johansen.evec[1:, 0] / rolling_johansen.evec[0, 0]\n",
    "        \n",
    "        # Calculate rolling spread\n",
    "        spread = np.zeros(window_size)\n",
    "        for j in range(window_data.shape[1]):\n",
    "            spread += rolling_johansen.evec[j, 0] * window_data[:, j]\n",
    "            \n",
    "        rolling_spreads[i] = spread[-1]  # Store only the last value\n",
    "        rolling_means[i] = np.mean(spread)\n",
    "        rolling_stds[i] = np.std(spread)\n",
    "    \n",
    "    # Plot rolling betas\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for i, name in enumerate(signal_names[1:]):\n",
    "        plt.plot(rolling_betas[:, i], label=f'Beta {name} vs Original', alpha=0.7)\n",
    "    plt.title('Rolling Hedge Ratios (Beta)')\n",
    "    plt.xlabel('Time Window')\n",
    "    plt.ylabel('Beta Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot rolling spread with bands\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(rolling_spreads, label='Spread', alpha=0.7)\n",
    "    plt.plot(rolling_means, label='Rolling Mean', linestyle='--', alpha=0.7)\n",
    "    plt.fill_between(range(len(rolling_spreads)), \n",
    "                     rolling_means - 2*rolling_stds,\n",
    "                     rolling_means + 2*rolling_stds,\n",
    "                     alpha=0.2, label='Â±2 Std Dev')\n",
    "    plt.title(f'Rolling Cointegration Spread (Window Size: {window_size} periods)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Spread')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final period statistics\n",
    "    print(\"\\nFINAL PERIOD STATISTICS:\")\n",
    "    print(\"Hedge Ratios (Beta):\")\n",
    "    for i, name in enumerate(signal_names[1:]):\n",
    "        print(f\"{name} vs Original: {rolling_betas[-1, i]:.4f}\")\n",
    "    print(f\"\\nSpread Mean: {rolling_means[-1]:.4f}\")\n",
    "    print(f\"Spread Std Dev: {rolling_stds[-1]:.4f}\")\n",
    "    current_zscore = (rolling_spreads[-1] - rolling_means[-1]) / rolling_stds[-1]\n",
    "    print(f\"Current Z-Score: {current_zscore[0]:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo cointegrating relationships found - cannot calculate hedge ratios and spread.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between bidPrice0 of backtest_3_df and backtest_4_df\n",
    "import numpy as np\n",
    "\n",
    "# Ensure both dataframes have the same length by taking the minimum\n",
    "min_length = min(len(backtest_3_df[0][0]), len(backtest_4_df[0][0]))\n",
    "print(min_length)\n",
    "backtest_3_prices = backtest_3_df[0][0]['bidPrice0'][:min_length]\n",
    "backtest_4_prices = backtest_4_df[0][0]['bidPrice0'][:min_length]\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(backtest_3_prices, backtest_4_prices)[0, 1]\n",
    "print(f\"Correlation between bidPrice0 of backtest_3_df and backtest_4_df: {correlation:.6f}\")\n",
    "\n",
    "# Plot the two bidPrices against each other\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot backtest_4_prices on the left y-axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Timestamp Event')\n",
    "ax1.set_ylabel('Backtest 4 bidPrice0 (Exchange 4, Security 1)', color=color)\n",
    "ax1.plot(backtest_3_df[0][0]['timestampEvent'][:min_length], backtest_4_prices, alpha=0.6, linewidth=1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for backtest_3_prices\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Backtest 3 bidPrice0 (Exchange 1, Security 3)', color=color)\n",
    "ax2.plot(backtest_3_df[0][0]['timestampEvent'][:min_length], backtest_3_prices, alpha=0.6, linewidth=1, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.title(f'Correlation between bidPrice0 of Different Exchanges\\nCorrelation: {correlation:.6f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_list = backtest.run(data_type='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MarketDataClient(bucket=\"gnome-market-data-dev\", aws_profile_name=\"AWSAdministratorAccess-443370708724\")\n",
    "client_data_params = {\n",
    "    \"exchange_id\": 1,\n",
    "    \"security_id\": 1,\n",
    "    \"start_datetime\": datetime.datetime(2025, 6, 7),\n",
    "    \"end_datetime\": datetime.datetime(2025, 6, 8),\n",
    "    \"schema_type\": SchemaType.MBP_10,\n",
    "}\n",
    "\n",
    "\n",
    "strategies = [Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_mean_500_delta'], minimum_ticker_cycle=3, starting_cash=100000.0)]\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta_alpha_00005'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta_alpha_0001'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_volatility_breakout'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_mean_reversion'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rsi_strategy'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_macd_strategy'], minimum_ticker_cycle=3, starting_cash=100000.0),\n",
    "            #   Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_moving_average_crossover'], minimum_ticker_cycle=3, starting_cash=100000.0)]\n",
    "# strategies = [Strategy(name=\"Simple Strategy\", action=global_actions['single_ticker_rolling_exp_mean_delta'], minimum_ticker_cycle=3, starting_cash=100000.0),]\n",
    "\n",
    "backtest_3 = Backtest(\n",
    "    client=client,\n",
    "    strategies=strategies,  # Pass a list of strategies\n",
    "    exchange_id=client_data_params[\"exchange_id\"],\n",
    "    security_id=client_data_params[\"security_id\"],\n",
    "    start_datetime=client_data_params[\"start_datetime\"],\n",
    "    end_datetime=client_data_params[\"end_datetime\"],\n",
    "    schema_type=client_data_params[\"schema_type\"]\n",
    ")\n",
    "backtest_3_df = backtest_3.run(data_type='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the rolling and exponentially weighted moving average (ewm) volatility of bidPrice0\n",
    "action_name = \"Volatility\"\n",
    "output_df = outputs_list[0][0]\n",
    "\n",
    "# Calculate rolling volatility\n",
    "rolling_volatility = output_df['bidPrice0'].rolling(window=20000).std()\n",
    "\n",
    "# Calculate exponentially weighted moving average volatility\n",
    "ewm_volatility = output_df['bidPrice0'].ewm(span=20000, adjust=False).std()\n",
    "\n",
    "# Classify each row into volatility categories based on ewm volatility\n",
    "volatility_thresholds = ewm_volatility.quantile([0.33, 0.66])\n",
    "output_df['volatility_category'] = pd.cut(\n",
    "    ewm_volatility,\n",
    "    bins=[-float('inf'), volatility_thresholds[0.33], volatility_thresholds[0.66], float('inf')],\n",
    "    labels=['low', 'medium', 'high']\n",
    ")\n",
    "\n",
    "# Plot the rolling and ewm volatility with zones\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(output_df['timestampEvent'], rolling_volatility, label='Rolling 100 Ticker Volatility of Bid Price', color='purple')\n",
    "plt.plot(output_df['timestampEvent'], ewm_volatility, label='EWM Volatility of Bid Price', color='orange')\n",
    "\n",
    "\n",
    "# Highlight the volatility zones\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'low', color='green', alpha=0.3, label='Low Volatility Zone')\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'medium', color='yellow', alpha=0.3, label='Medium Volatility Zone')\n",
    "plt.fill_between(output_df['timestampEvent'], 0, ewm_volatility, where=output_df['volatility_category'] == 'high', color='red', alpha=0.3, label='High Volatility Zone')\n",
    "\n",
    "plt.xlabel('Timestamp Event')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title(f'Rolling and EWM Volatility of Bid Price for {action_name}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% cell 4.5 code\n",
    "\n",
    "# Plot bidPrice0 below the volatility graph\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(output_df['timestampEvent'], output_df['bidPrice0'], label='Bid Price', color='blue')\n",
    "plt.xlabel('Timestamp Event')\n",
    "plt.ylabel('Bid Price')\n",
    "plt.title('Bid Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration Trading Strategy Explanation\n",
    "\n",
    "print(\"=== COINTEGRATION TRADING STRATEGY EXPLANATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"HIGH-LEVEL OVERVIEW:\")\n",
    "print(\"Cointegration is a statistical relationship between two or more time series that\")\n",
    "print(\"tend to move together over the long term, even though they may diverge in the short term.\")\n",
    "print(\"As a hedge fund manager, this creates pairs trading opportunities where we can profit\")\n",
    "print(\"from temporary divergences while maintaining market-neutral exposure.\")\n",
    "print()\n",
    "\n",
    "print(\"DETAILED EXPLANATION:\")\n",
    "print()\n",
    "\n",
    "print(\"1. CONCEPT:\")\n",
    "print(\"   - Two assets are cointegrated if their price difference is stationary\")\n",
    "print(\"   - Even if individual prices are non-stationary (trending), their spread reverts to mean\")\n",
    "print(\"   - Creates predictable trading opportunities when the spread deviates from equilibrium\")\n",
    "print()\n",
    "\n",
    "print(\"2. MATHEMATICAL FOUNDATION:\")\n",
    "print(\"   - If Price_A and Price_B are cointegrated:\")\n",
    "print(\"   - Spread = Price_A - Î² Ã— Price_B (where Î² is the hedge ratio)\")\n",
    "print(\"   - The spread follows a mean-reverting process\")\n",
    "print(\"   - When spread > mean + threshold: Short A, Long B\")\n",
    "print(\"   - When spread < mean - threshold: Long A, Short B\")\n",
    "print()\n",
    "\n",
    "print(\"3. TRADING MECHANISM:\")\n",
    "print(\"   Step 1: Identify cointegrated pairs using statistical tests (Engle-Granger, Johansen)\")\n",
    "print(\"   Step 2: Calculate the hedge ratio (Î²) using regression or error correction models\")\n",
    "print(\"   Step 3: Monitor the spread = Price_A - Î² Ã— Price_B\")\n",
    "print(\"   Step 4: Enter positions when spread deviates significantly from mean\")\n",
    "print(\"   Step 5: Exit when spread reverts to mean\")\n",
    "print()\n",
    "\n",
    "print(\"4. HEDGE FUND IMPLEMENTATION:\")\n",
    "print(\"   ADVANTAGES:\")\n",
    "print(\"   - Market-neutral strategy (hedged against market direction)\")\n",
    "print(\"   - Statistical edge based on mean reversion\")\n",
    "print(\"   - Lower volatility than directional strategies\")\n",
    "print(\"   - Scalable across multiple pairs\")\n",
    "print(\"   - Works in various market conditions\")\n",
    "print()\n",
    "print(\"   CHALLENGES:\")\n",
    "print(\"   - Cointegration relationships can break down\")\n",
    "print(\"   - Requires sophisticated statistical analysis\")\n",
    "print(\"   - Transaction costs can erode profits\")\n",
    "print(\"   - Need real-time monitoring of multiple pairs\")\n",
    "print(\"   - Risk of prolonged divergence before convergence\")\n",
    "print()\n",
    "\n",
    "print(\"5. REAL-WORLD EXAMPLE - CURRENCY PAIRS:\")\n",
    "print(\"   Consider EUR/USD and GBP/USD:\")\n",
    "print(\"   - Both influenced by USD strength/weakness\")\n",
    "print(\"   - Economic ties between EUR and GBP create cointegration\")\n",
    "print()\n",
    "print(\"   Historical Analysis:\")\n",
    "print(\"   - EUR/USD = 1.1000, GBP/USD = 1.3000\")\n",
    "print(\"   - Hedge ratio (Î²) = 0.85 (from regression analysis)\")\n",
    "print(\"   - Normal spread = 1.1000 - 0.85 Ã— 1.3000 = 0.0950\")\n",
    "print()\n",
    "print(\"   Trading Opportunity:\")\n",
    "print(\"   - Current: EUR/USD = 1.0800, GBP/USD = 1.3200\")\n",
    "print(\"   - Current spread = 1.0800 - 0.85 Ã— 1.3200 = -0.0420\")\n",
    "print(\"   - Spread is 0.137 below normal (significant deviation)\")\n",
    "print(\"   - Trade: Long EUR/USD, Short GBP/USD\")\n",
    "print(\"   - Exit when spread returns to ~0.095\")\n",
    "print()\n",
    "\n",
    "print(\"6. STATISTICAL TESTS & IMPLEMENTATION:\")\n",
    "print(\"   A. Engle-Granger Test:\")\n",
    "print(\"      - Regress Price_A on Price_B\")\n",
    "print(\"      - Test residuals for stationarity using ADF test\")\n",
    "print(\"      - If residuals are stationary, pairs are cointegrated\")\n",
    "print()\n",
    "print(\"   B. Johansen Test:\")\n",
    "print(\"      - Tests for multiple cointegrating relationships\")\n",
    "print(\"      - More robust for multiple time series\")\n",
    "print(\"      - Provides cointegrating vectors directly\")\n",
    "print()\n",
    "print(\"   C. Error Correction Model:\")\n",
    "print(\"      - Î”Price_A = Î± Ã— (Price_A - Î² Ã— Price_B)_{t-1} + noise\")\n",
    "print(\"      - Î± measures speed of mean reversion\")\n",
    "print(\"      - Higher |Î±| indicates faster convergence\")\n",
    "print()\n",
    "\n",
    "print(\"7. RISK MANAGEMENT:\")\n",
    "print(\"   - Position sizing based on historical volatility of spread\")\n",
    "print(\"   - Stop-loss when spread exceeds historical extremes\")\n",
    "print(\"   - Regular re-estimation of cointegration parameters\")\n",
    "print(\"   - Monitoring for structural breaks in relationships\")\n",
    "print(\"   - Diversification across multiple cointegrated pairs\")\n",
    "print()\n",
    "\n",
    "print(\"8. ADVANCED APPLICATIONS:\")\n",
    "print(\"   A. Multi-Asset Cointegration:\")\n",
    "print(\"      - Basket of currencies vs. single currency\")\n",
    "print(\"      - Sector rotation strategies\")\n",
    "print(\"      - Cross-asset cointegration (bonds vs. stocks)\")\n",
    "print()\n",
    "print(\"   B. Dynamic Hedge Ratios:\")\n",
    "print(\"      - Kalman filters for time-varying relationships\")\n",
    "print(\"      - Rolling window estimation\")\n",
    "print(\"      - Regime-switching models\")\n",
    "print()\n",
    "print(\"   C. High-Frequency Implementation:\")\n",
    "print(\"      - Intraday cointegration patterns\")\n",
    "print(\"      - Tick-by-tick spread monitoring\")\n",
    "print(\"      - Latency arbitrage opportunities\")\n",
    "print()\n",
    "\n",
    "print(\"9. COMPARISON TO TRIANGULAR ARBITRAGE:\")\n",
    "print(\"   TRIANGULAR ARBITRAGE:\")\n",
    "print(\"   - Risk-free, immediate profit\")\n",
    "print(\"   - Requires perfect execution\")\n",
    "print(\"   - Opportunities are rare and fleeting\")\n",
    "print()\n",
    "print(\"   COINTEGRATION TRADING:\")\n",
    "print(\"   - Statistical edge, not risk-free\")\n",
    "print(\"   - Longer holding periods\")\n",
    "print(\"   - More frequent opportunities\")\n",
    "print(\"   - Requires sophisticated modeling\")\n",
    "print()\n",
    "\n",
    "print(\"10. PRACTICAL EXAMPLE - IMPLEMENTATION STEPS:\")\n",
    "print(\"    # Step 1: Data Collection\")\n",
    "print(\"    eur_usd = get_price_data('EUR/USD')\")\n",
    "print(\"    gbp_usd = get_price_data('GBP/USD')\")\n",
    "print()\n",
    "print(\"    # Step 2: Test for Cointegration\")\n",
    "print(\"    from statsmodels.tsa.stattools import coint\")\n",
    "print(\"    score, pvalue, _ = coint(eur_usd, gbp_usd)\")\n",
    "print()\n",
    "print(\"    # Step 3: Calculate Hedge Ratio\")\n",
    "print(\"    hedge_ratio = np.polyfit(gbp_usd, eur_usd, 1)[0]\")\n",
    "print()\n",
    "print(\"    # Step 4: Calculate Spread\")\n",
    "print(\"    spread = eur_usd - hedge_ratio * gbp_usd\")\n",
    "print()\n",
    "print(\"    # Step 5: Generate Signals\")\n",
    "print(\"    z_score = (spread - spread.mean()) / spread.std()\")\n",
    "print(\"    signals = np.where(z_score > 2, -1,  # Short spread\")\n",
    "print(\"                      np.where(z_score < -2, 1, 0))  # Long spread\")\n",
    "print()\n",
    "\n",
    "print(\"As a hedge fund manager, cointegration strategies offer a more sustainable\")\n",
    "print(\"approach than pure arbitrage. While not risk-free, they provide statistical\")\n",
    "print(\"edges that can be systematically exploited with proper risk management,\")\n",
    "print(\"making them a cornerstone of many quantitative hedge fund strategies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = outputs_list[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create the plot for buy/sell signals for each strategy\n",
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    # Sample every 1000th row for the current strategy's output\n",
    "    sampled_df = output_df.iloc[::1000, :]\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=sampled_df, x='timestampEvent', y='bidPrice0', label='Bid Price')\n",
    "    sns.lineplot(data=sampled_df, x='timestampEvent', y='askPrice0', label='Ask Price')\n",
    "    plt.xlabel('Timestamp Event')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Bid and Ask Prices')\n",
    "\n",
    "\n",
    "    # Add green and red dots for actions specific to each strategy\n",
    "    buy_signals = output_df[output_df[f'{action_name}_cash_action'] > 0]\n",
    "    sell_signals = output_df[output_df[f'{action_name}_cash_action'] < 0]\n",
    "\n",
    "    plt.scatter(buy_signals['timestampEvent'], buy_signals['askPrice0'], color='green', label='Buy Signal', marker='o')\n",
    "    plt.scatter(sell_signals['timestampEvent'], sell_signals['askPrice0'], color='red', label='Sell Signal', marker='x')\n",
    "\n",
    "    plt.xlabel('Timestamp Event')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Buy/Sell Signals for {action_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot Equity Position on the first y-axis\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Equity Position', color='tab:blue')\n",
    "    sns.lineplot(data=output_df, x=output_df.index, y=f'{action_name}_equity_position', label='Equity Position', ax=ax1, color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    # Create a second y-axis for Cash Balance\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Cash Balance', color='tab:orange')\n",
    "    sns.lineplot(data=output_df, x=output_df.index, y=f'{action_name}_cash_balance', label='Cash Balance', ax=ax2, color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Add title and legend\n",
    "    fig.suptitle(f'Equity Position and Cash Balance Over Time for {action_name}')\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store metrics for each strategy\n",
    "strategy_metrics = {}\n",
    "\n",
    "for strategy, outputs in zip(strategies, outputs_list):\n",
    "    action_name = strategy.action.name\n",
    "    output_df = outputs[0]\n",
    "\n",
    "    # Classify each row into volatility categories based on ewm volatility\n",
    "    ewm_volatility = output_df['bidPrice0'].ewm(span=20000, adjust=False).std()\n",
    "    volatility_thresholds = ewm_volatility.quantile([0.33, 0.66])\n",
    "    output_df['volatility_category'] = pd.cut(\n",
    "        ewm_volatility,\n",
    "        bins=[-float('inf'), volatility_thresholds[0.33], volatility_thresholds[0.66], float('inf')],\n",
    "        labels=['low', 'medium', 'high']\n",
    "    )\n",
    "\n",
    "    # Initialize a dictionary to store metrics for each volatility zone\n",
    "    strategy_metrics[action_name] = {}\n",
    "\n",
    "    # Calculate and store advanced HFT metrics for each volatility zone\n",
    "    for volatility_zone in ['low', 'medium', 'high']:\n",
    "        zone_df = output_df[output_df['volatility_category'] == volatility_zone]\n",
    "\n",
    "        # Identify sectors within the volatility zone\n",
    "        zone_df['sector'] = (zone_df.index.to_series().diff() != 1).cumsum()\n",
    "\n",
    "        # Calculate metrics at the sector level\n",
    "        sector_metrics = []\n",
    "        for sector, sector_df in zone_df.groupby('sector'):\n",
    "            mean_bid_price = sector_df['bidPrice0'].mean()\n",
    "            mean_ask_price = sector_df['askPrice0'].mean()\n",
    "            total_trades = len(sector_df[sector_df[f'{action_name}_cash_action'] != 0])\n",
    "            profit_factor = (sector_df[f'{action_name}_cash_balance'].iloc[-1] - strategy.starting_cash) / strategy.starting_cash if strategy.starting_cash != 0 else float('inf')\n",
    "            number_of_buys = len(sector_df[sector_df[f'{action_name}_cash_action'] > 0])\n",
    "            number_of_sells = len(sector_df[sector_df[f'{action_name}_cash_action'] < 0])\n",
    "\n",
    "            sector_metrics.append({\n",
    "                'mean_bid_price': mean_bid_price,\n",
    "                'mean_ask_price': mean_ask_price,\n",
    "                'total_trades': total_trades,\n",
    "                'profit_factor': profit_factor,\n",
    "                'number_of_buys': number_of_buys,\n",
    "                'number_of_sells': number_of_sells\n",
    "            })\n",
    "        \n",
    "    \n",
    "        # Calculate mean and std of the sector metrics at the volatility level\n",
    "        mean_metrics = {key: np.mean([sector[key] for sector in sector_metrics]) for key in sector_metrics[0]}\n",
    "        std_metrics = {key: np.std([sector[key] for sector in sector_metrics]) for key in sector_metrics[0]}\n",
    "\n",
    "        # Store the metrics for each strategy and volatility zone\n",
    "        strategy_metrics[action_name][volatility_zone] = {\n",
    "            'mean_metrics': mean_metrics,\n",
    "            'std_metrics': std_metrics,\n",
    "            \"all_metrics\": sector_metrics\n",
    "        }\n",
    "\n",
    "# Now strategy_metrics contains all the metrics for each strategy and volatility zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from strategy_metrics for each strategy and volatility zone\n",
    "all_metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': strategy,\n",
    "        'Volatility': volatility_zone,\n",
    "        'Profit Factor': metric['profit_factor']\n",
    "    }\n",
    "    for strategy in strategy_metrics\n",
    "    for volatility_zone in strategy_metrics[strategy]\n",
    "    for metric in strategy_metrics[strategy][volatility_zone]['all_metrics']\n",
    "])\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the DataFrame using the mean and std metrics\n",
    "profit_factors_df = all_metrics_df\n",
    "\n",
    "# Set the theme for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the figure with increased height to space strategies more\n",
    "f, ax = plt.subplots(figsize=(8, 12))  # Adjust the height to space out the strategies\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Define a custom palette for volatility levels\n",
    "custom_palette = {'low': 'green', 'medium': 'yellow', 'high': 'red'}\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(\n",
    "    data=profit_factors_df, x=\"Profit Factor\", y=\"Strategy\", hue=\"Volatility\",\n",
    "    dodge=True, alpha=.25, zorder=1, palette=custom_palette\n",
    ")\n",
    "\n",
    "# Add mean and std as black font labels next to each stripplot for each volatility zone\n",
    "for i, (strategy, strategy_df) in enumerate(profit_factors_df.groupby('Strategy')):\n",
    "    # Ensure the subplot's stripplot strategy matches the strategy of the mean being printed\n",
    "    strategy_index = profit_factors_df['Strategy'].unique().tolist().index(strategy)\n",
    "    for j, (volatility, volatility_df) in enumerate(strategy_df.groupby('Volatility')):\n",
    "        strategy_volatility_data = volatility_df['Profit Factor']\n",
    "        mean = strategy_volatility_data.mean()\n",
    "        std = strategy_volatility_data.std()\n",
    "        # Align the text horizontally with the respective volatility zones\n",
    "        ax.text(2.9, strategy_index + j * 0.2 - 0.2, f'Mean: {mean:.2f} +/- {std:.2f}', color='black', ha='right', va='top', fontsize='small')\n",
    "\n",
    "# Add a bold black vertical line at 1\n",
    "ax.axvline(x=1, color='black', linewidth=2, linestyle='-')\n",
    "\n",
    "# Set the x-axis limits from 0 to 2\n",
    "ax.set_xlim(-1, 3)\n",
    "\n",
    "# Add horizontal lines to separate each row\n",
    "for ytick in range(len(ax.get_yticks()) + 1):\n",
    "    ax.axhline(y=ytick - 0.5, color='black', linewidth=0.5, linestyle='-')\n",
    "\n",
    "# Add the legend manually since move_legend requires an existing legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, loc=\"lower right\", ncol=3, frameon=True, columnspacing=1, handletextpad=0, fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
